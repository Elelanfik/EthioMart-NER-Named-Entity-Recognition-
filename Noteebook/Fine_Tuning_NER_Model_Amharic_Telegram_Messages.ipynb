{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "notebook_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, '..')))\n",
    "sys.path.append(os.path.abspath('../scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "notebook_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, '..')))\n",
    "sys.path.append(os.path.abspath('../scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['INIMA', 'JAPAN', 'COFFEE', 'GRINDER', 'ከፍተኛ', 'ጥራት', '150w', 'ቋቱ', 'እና', 'ምላጩ', 'የብረት', 'made', 'in', 'japan', 'ዋጋ', '1400', 'ብር', 'ውስን', 'ፍሬ', 'ነው', 'ያለው', 'አድራሻ', 'ቁ1', 'መገናኛ', 'ስሪ', 'ኤም', 'ሲቲ', 'ሞል', 'ሁለተኛ', 'ፎቅ', 'ቢሮ', 'ቁ', 'SL05Aከ', 'ሊፍቱ', 'ፊት', 'ለ', 'ፊት', 'ቁ2', 'ለቡ', 'መዳህኒዓለም', 'ቤተክርስቲያን', 'ፊት', 'ለፊት', 'ዛምሞል', '2ኛ', 'ፎቅ', 'ቢሮ', 'ቁጥር214', '0909522840', '0923350054', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'shageronlinestore', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን', 'httpstmeShageronlinestore'], ['I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-LOC', 'I-PRODUCT', 'I-PRODUCT', 'I-LOC', 'I-PRODUCT', 'I-LOC', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRICE', 'I-PRICE', 'I-PRICE', 'I-PRODUCT', 'I-LOC', 'I-PRODUCT', 'I-PRODUCT', 'B-LOC', 'I-PRODUCT', 'I-LOC', 'I-LOC', 'I-LOC', 'I-PRODUCT', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-PRODUCT', 'B-LOC', 'I-LOC', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'B-LOC', 'I-LOC', 'I-LOC', 'I-PRODUCT', 'I-PRODUCT', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-LOC', 'I-LOC', 'I-LOC', 'I-PRODUCT', 'I-PRODUCT'])\n"
     ]
    }
   ],
   "source": [
    "rpath=r\"C:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\"\n",
    "def read_conll_data(file_path):\n",
    "    sentences = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line == \"\":  # End of a sentence\n",
    "                if current_tokens:\n",
    "                    sentences.append((current_tokens, current_labels))\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "            else:\n",
    "                token_label_pair = line.split()  # Split by space (or change to tab if needed)\n",
    "                if len(token_label_pair) == 2:\n",
    "                    token, label = token_label_pair\n",
    "                    current_tokens.append(token)\n",
    "                    current_labels.append(label)\n",
    "\n",
    "        # Append the last sentence if the file doesn't end with an empty line\n",
    "        if current_tokens:\n",
    "            sentences.append((current_tokens, current_labels))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Example usage:\n",
    "file_path = os.path.join(rpath, 'Data','labeled_telegram_data_conll.conll')\n",
    "# Create DataFrame\n",
    "conll_data = read_conll_data(file_path)\n",
    "\n",
    "# Print first sentence (tokens and labels)\n",
    "print(conll_data[0])  # Example output: (['token1', 'token2'], ['label1', 'label2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INIMA, JAPAN, COFFEE, GRINDER, ከፍተኛ, ጥራት, 150...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INIMA, JAPAN, COFFEE, GRINDER, ከፍተኛ, ጥራት, 150...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[stainless, still, flower, shape, cake, mold, ...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Delux, Foldable, multifunctional, Draying, RA...</td>\n",
       "      <td>[I-PRODUCT, B-PRODUCT, I-PRODUCT, I-PRODUCT, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[አልቆልለተባላችሁበድጋሚአስገብተናል, Automatic, rotating, n...</td>\n",
       "      <td>[I-LOC, I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, የልጆች, የንባብ, እን...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, ለፀጉርዎ, ልስላሳሴ, ...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, ለሁለቱም, ፆታ, የሚሆ...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, ከሸራ, የተሰራ, የልጆ...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, እንዲህ, ድምቅ, ያለ,...</td>\n",
       "      <td>[I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tokens  \\\n",
       "0    [INIMA, JAPAN, COFFEE, GRINDER, ከፍተኛ, ጥራት, 150...   \n",
       "1    [INIMA, JAPAN, COFFEE, GRINDER, ከፍተኛ, ጥራት, 150...   \n",
       "2    [stainless, still, flower, shape, cake, mold, ...   \n",
       "3    [Delux, Foldable, multifunctional, Draying, RA...   \n",
       "4    [አልቆልለተባላችሁበድጋሚአስገብተናል, Automatic, rotating, n...   \n",
       "..                                                 ...   \n",
       "995  [እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, የልጆች, የንባብ, እን...   \n",
       "996  [እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, ለፀጉርዎ, ልስላሳሴ, ...   \n",
       "997  [እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, ለሁለቱም, ፆታ, የሚሆ...   \n",
       "998  [እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, ከሸራ, የተሰራ, የልጆ...   \n",
       "999  [እንኳን, ለአዲሱ, ዓመት, በሰላም, አደረሳችሁ, እንዲህ, ድምቅ, ያለ,...   \n",
       "\n",
       "                                                labels  \n",
       "0    [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "1    [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "2    [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "3    [I-PRODUCT, B-PRODUCT, I-PRODUCT, I-PRODUCT, B...  \n",
       "4    [I-LOC, I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRO...  \n",
       "..                                                 ...  \n",
       "995  [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "996  [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "997  [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "998  [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "999  [I-PRODUCT, I-PRODUCT, I-PRODUCT, I-PRODUCT, I...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "tokens, labels = zip(*conll_data)\n",
    "telegram_df = pd.DataFrame({'tokens': tokens, 'labels': labels})\n",
    "telegram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: {0, 1, 2, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "# Define all labels explicitly\n",
    "label_encoding = {\n",
    "    'O': 0, \n",
    "    'B-PRODUCT': 1, \n",
    "    'B-Product': 1,  # Ensure both spellings are included\n",
    "    'I-PRODUCT': 2, \n",
    "    'I-PHONE': 3, \n",
    "    'I-LOC': 4, \n",
    "    'B-LOC': 4,  # Explicitly define 'B-LOC' and 'I-LOC' as the same value\n",
    "    'I-PRICE': 5, \n",
    "    'B-PRICE': 5  # Ensure 'B-PRICE' is also included\n",
    "}\n",
    "\n",
    "# Apply label encoding to the 'labels' column\n",
    "telegram_df['labels'] = telegram_df['labels'].apply(lambda x: [label_encoding.get(label, -1) for label in x])\n",
    "\n",
    "# Check unique labels after encoding\n",
    "unique_labels = set([label for sublist in telegram_df['labels'] for label in sublist])\n",
    "print(\"Unique labels in the dataset:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\\Week-5\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast  # For safely evaluating list strings if needed\n",
    "from datasets import load_dataset,Dataset\n",
    "\n",
    "# Ensure that 'tokens' and 'labels' columns are lists\n",
    "telegram_df['tokens'] = telegram_df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "telegram_df['labels'] = telegram_df['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(telegram_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 800\n",
      "Validation dataset size: 100\n",
      "Test dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.2)  # 80% train, 20% remaining\n",
    "train_dataset = train_test_split['train']\n",
    "remaining_dataset = train_test_split['test']\n",
    "\n",
    "# Further split the remaining dataset into validation and test sets (50% of remaining for each)\n",
    "val_test_split = remaining_dataset.train_test_split(test_size=0.5)  # 50% for validation, 50% for testing\n",
    "eval_dataset = val_test_split['train']  # Validation set\n",
    "test_dataset = val_test_split['test']    # Test set\n",
    "\n",
    "# Output the sizes of the datasets\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(eval_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: {0, 1, 2, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "for example in train_dataset:\n",
    "    unique_labels.update(example['labels'])\n",
    "print(\"Unique labels in the dataset:\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\\Week-5\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fikad\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "label_all_tokens = False\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens like [CLS], [SEP]\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(label):  # Ensure valid index for the label\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)  # Handle index out of bounds\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 828.83 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 742.12 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 682.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\\Week-5\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m DataLoader(tokenized_eval_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Setup training arguments\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCustomTrainer\u001b[39;00m(Trainer):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_train_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\\Week-5\\lib\\site-packages\\transformers\\training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1772\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32mc:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\\Week-5\\lib\\site-packages\\transformers\\training_args.py:2294\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\\Week-5\\lib\\site-packages\\transformers\\utils\\generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\fikad\\Desktop\\10acedamy\\EthioMart-NER-Named-Entity-Recognition-\\Week-5\\lib\\site-packages\\transformers\\training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2170\u001b[0m         )\n\u001b[0;32m   2171\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load a pre-trained token classification model (XLM-RoBERTa)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\", num_labels=6)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    padded_input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    max_length = padded_input_ids.size(1)\n",
    "    padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    padded_labels = torch.nn.functional.pad(padded_labels, (0, max_length - padded_labels.size(1)), value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': padded_input_ids,\n",
    "        'attention_mask': padded_attention_mask,\n",
    "        'labels': padded_labels,\n",
    "    }\n",
    "\n",
    "# Create DataLoader with the custom collate function\n",
    "train_loader = DataLoader(tokenized_train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "eval_loader = DataLoader(tokenized_eval_dataset, batch_size=16, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return train_loader\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        return eval_loader\n",
    "\n",
    "\n",
    "# Initialize the CustomTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(rpath, 'models','xlmBert_fine_tuned_model')\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "model_path = os.path.join(rpath, 'models','xlmBert_fine_tuned_tokenizer')\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(trainer, eval_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in eval_loader:\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = model(**{k: v.to(trainer.model.device) for k, v in batch.items()})\n",
    "            logits = outputs.logits\n",
    "\n",
    "        predicted = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "        # Append to lists\n",
    "        predictions.append(predicted)\n",
    "        true_labels.append(labels)\n",
    "\n",
    "    return predictions, true_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Week-5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
